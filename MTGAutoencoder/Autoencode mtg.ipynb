{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "cards = requests.get(\"https://c2.scryfall.com/file/scryfall-bulk/oracle-cards/oracle-cards-20201024210538.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splice_reminders(otext):\n",
    "    while True:\n",
    "        start_index = otext.find('(')\n",
    "        if start_index == -1:\n",
    "            break\n",
    "        end_index = otext.find(')')\n",
    "        otext = otext[0:start_index] + otext[end_index+1:]\n",
    "    return otext\n",
    "\n",
    "def extract_data(card):\n",
    "    if card['layout'] in [\"art_series\", \"double_faced_token\", \"transform\", \"split\", \"adventure\", \"modal_dfc\", \"flip\"]:\n",
    "        return None\n",
    "    if card['set'] in [\"unh\", \"ugl\", \"ust\", \"und\", 'wc00', 'wc01', 'wc02', 'wc03', 'wc04', 'wc97', 'wc98', 'wc99', 'h17', 'ptg', 'cmb1', 'mznr']:\n",
    "        #print(card[\"name\"])\n",
    "        return None\n",
    "    if \"oracle_text\" not in card:\n",
    "        print(card)\n",
    "    if card[\"type_line\"] == \"Card\":\n",
    "        return None\n",
    "    stats = [\n",
    "        card[\"type_line\"],\n",
    "        splice_reminders(card[\"oracle_text\"])\n",
    "    ]\n",
    "    if \"mana_cost\" in card:\n",
    "        stats.append(card[\"mana_cost\"])\n",
    "    if \"power\" in card:\n",
    "        stats.append(f\"power: {card['power']}\")\n",
    "        stats.append(f\"toughness: {card['toughness']}\")\n",
    "    if \"loyalty\" in card:\n",
    "        stats.append(f\"loyalty: {card['loyalty']}\")\n",
    "    \n",
    "    text = \"|\".join(stats)\n",
    "    if(len(text) > 200):\n",
    "        return None\n",
    "    return text.replace(card['name'], \"$\").lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_strings = [extract_data(c) for c in cards if extract_data(c) is not None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(200, 'enchantment|whenever a player casts a creature spell, destroy all reflections. then that player creates an x/x white reflection creature token, where x is the converted mana cost of that spell.|{2}{w}'), (200, 'sorcery|this spell costs {3} less to cast if it targets a creature whose controller has eight or more cards in their graveyard.\\ngain control of target creature with converted mana cost x.|{x}{u}{u}{u}'), (200, 'sorcery|\\nsearch your library for a card with converted mana cost less than or equal to the number of lands you control, reveal it, and put it into your hand. then shuffle your library.|{2/b}{2/b}{2/b}'), (200, 'enchantment|landfall — whenever a land enters the battlefield under your control, choose one —\\n• create a 1/1 white kor ally creature token.\\n• creatures you control get +1/+1 until end of turn.|{3}{w}'), (200, 'artifact|{3}, {t}: choose a number greater than 0 and a color. target opponent reveals their hand. if that opponent reveals exactly the chosen number of cards of the chosen color, you draw a card.|{2}')]\n"
     ]
    }
   ],
   "source": [
    "longest = list(reversed(sorted([(len(s), s) for s in card_strings], key=lambda x: x[0])))\n",
    "print(longest[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(set([c for s in card_strings for c in s]))\n",
    "tokens.insert(0, '<PAD>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import *\n",
    "from keras import layers\n",
    "from keras_layer_normalization import LayerNormalization\n",
    "from keras_multi_head import MultiHeadAttention\n",
    "from keras_position_wise_feed_forward import FeedForward\n",
    "from keras_pos_embd import TrigPosEmbedding\n",
    "from keras_embed_sim import EmbeddingRet, EmbeddingSim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(token_num,\n",
    "              embed_dim,\n",
    "              encoder_num,\n",
    "              decoder_num,\n",
    "              head_num,\n",
    "              hidden_dim,\n",
    "              attention_activation=None,\n",
    "              feed_forward_activation=gelu,\n",
    "              dropout_rate=0.0,\n",
    "              use_same_embed=True,\n",
    "              embed_weights=None,\n",
    "              embed_trainable=None,\n",
    "              trainable=True):\n",
    "    \"\"\"Get full model without compilation.\n",
    "    :param token_num: Number of distinct tokens.\n",
    "    :param embed_dim: Dimension of token embedding.\n",
    "    :param encoder_num: Number of encoder components.\n",
    "    :param decoder_num: Number of decoder components.\n",
    "    :param head_num: Number of heads in multi-head self-attention.\n",
    "    :param hidden_dim: Hidden dimension of feed forward layer.\n",
    "    :param attention_activation: Activation for multi-head self-attention.\n",
    "    :param feed_forward_activation: Activation for feed-forward layer.\n",
    "    :param dropout_rate: Dropout rate.\n",
    "    :param use_same_embed: Whether to use the same token embedding layer. `token_num`, `embed_weights` and\n",
    "                           `embed_trainable` should be lists of two elements if it is False.\n",
    "    :param embed_weights: Initial weights of token embedding.\n",
    "    :param embed_trainable: Whether the token embedding is trainable. It will automatically set to False if the given\n",
    "                            value is None when embedding weights has been provided.\n",
    "    :param trainable: Whether the layers are trainable.\n",
    "    :return: Keras model.\n",
    "    \"\"\"\n",
    "    if not isinstance(token_num, list):\n",
    "        token_num = [token_num, token_num]\n",
    "    encoder_token_num, decoder_token_num = token_num\n",
    "\n",
    "    if not isinstance(embed_weights, list):\n",
    "        embed_weights = [embed_weights, embed_weights]\n",
    "    encoder_embed_weights, decoder_embed_weights = embed_weights\n",
    "    if encoder_embed_weights is not None:\n",
    "        encoder_embed_weights = [encoder_embed_weights]\n",
    "    if decoder_embed_weights is not None:\n",
    "        decoder_embed_weights = [decoder_embed_weights]\n",
    "\n",
    "    if not isinstance(embed_trainable, list):\n",
    "        embed_trainable = [embed_trainable, embed_trainable]\n",
    "    encoder_embed_trainable, decoder_embed_trainable = embed_trainable\n",
    "    if encoder_embed_trainable is None:\n",
    "        encoder_embed_trainable = encoder_embed_weights is None\n",
    "    if decoder_embed_trainable is None:\n",
    "        decoder_embed_trainable = decoder_embed_weights is None\n",
    "\n",
    "    \n",
    "    encoder_embed_layer = decoder_embed_layer = EmbeddingRet(\n",
    "        input_dim=encoder_token_num,\n",
    "        output_dim=embed_dim,\n",
    "        mask_zero=False,\n",
    "        weights=encoder_embed_weights,\n",
    "        trainable=encoder_embed_trainable,\n",
    "        name='Token-Embedding',\n",
    "        input_length=200\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    encoder_input = keras.layers.Input(shape=(None,), name='Encoder-Input')\n",
    "    encoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Encoder-Embedding',\n",
    "    )(encoder_embed_layer(encoder_input)[0])\n",
    "    print(f\"[{[eel.shape for eel in encoder_embed_layer(encoder_input)]}]\")\n",
    "    encoded_layer = get_encoders(\n",
    "        encoder_num=encoder_num,\n",
    "        input_layer=encoder_embed,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    print(encoded_layer.shape)\n",
    "    resh = layers.Reshape((200*12,))(encoded_layer)\n",
    "    print(resh.shape)\n",
    "    #flat = layers.GlobalAveragePooling1D()(resh)\n",
    "    encoded_layer_dense = layers.Constant(10)(resh)\n",
    "    #layers.Dense(12, activation='relu', name='compr')(resh)\n",
    "    \n",
    "    dld = layers.Dense(200*12, activation='relu', name='dld')(encoded_layer_dense)\n",
    "    print(dld.shape)\n",
    "    dldresh = layers.Reshape((200, 12))(dld)\n",
    "    print(dldresh.shape)\n",
    "    \n",
    "    decoder_input = keras.layers.Input(shape=(None,), name='Decoder-Input')\n",
    "    decoder_embed, decoder_embed_weights = decoder_embed_layer(decoder_input)\n",
    "    decoder_embed = TrigPosEmbedding(\n",
    "        mode=TrigPosEmbedding.MODE_ADD,\n",
    "        name='Decoder-Embedding',\n",
    "    )(decoder_embed)\n",
    "    decoded_layer = get_decoders(\n",
    "        decoder_num=decoder_num,\n",
    "        input_layer=decoder_embed,\n",
    "        encoded_layer=dldresh,\n",
    "        head_num=head_num,\n",
    "        hidden_dim=hidden_dim,\n",
    "        attention_activation=attention_activation,\n",
    "        feed_forward_activation=feed_forward_activation,\n",
    "        dropout_rate=dropout_rate,\n",
    "        trainable=trainable,\n",
    "    )\n",
    "    output_layer = EmbeddingSim(\n",
    "        trainable=trainable,\n",
    "        name='Decoder-Output',\n",
    "    )([decoded_layer, decoder_embed_weights])\n",
    "    return keras.models.Model(inputs=[encoder_input, decoder_input], outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[TensorShape([Dimension(None), Dimension(None), Dimension(12)]), TensorShape([Dimension(57), Dimension(12)])]]\n",
      "(?, 200, 12)\n",
      "(?, 2400)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.layers' has no attribute 'Constant'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-db960611cb40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mfeed_forward_activation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdropout_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0membed_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m model.compile(\n",
      "\u001b[1;32m<ipython-input-82-ff5bf1e3bf98>\u001b[0m in \u001b[0;36mget_model\u001b[1;34m(token_num, embed_dim, encoder_num, decoder_num, head_num, hidden_dim, attention_activation, feed_forward_activation, dropout_rate, use_same_embed, embed_weights, embed_trainable, trainable)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m#flat = layers.GlobalAveragePooling1D()(resh)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mencoded_layer_dense\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     \u001b[1;31m#layers.Dense(12, activation='relu', name='compr')(resh)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'keras.layers' has no attribute 'Constant'"
     ]
    }
   ],
   "source": [
    "model = get_model(\n",
    "    token_num=len(tokens),\n",
    "    embed_dim=12,\n",
    "    encoder_num=3,\n",
    "    decoder_num=2,\n",
    "    head_num=3,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((len(tokens), 12)),\n",
    ")\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_card_string(tokens, card_string, pad_length=200):\n",
    "    return [tokens.index(c) for c in card_string] + [0] * (pad_length - len(card_string))\n",
    "\n",
    "def encode_card_string_nested(tokens, card_string, pad_length=200):\n",
    "    return [[tokens.index(c)] for c in card_string] + [[0]] * (pad_length - len(card_string))\n",
    "\n",
    "def decode_card_string(tokens, enc_card_string):\n",
    "    return \"\".join([tokens[i] for i in enc_card_string])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "(16209, 200)\n"
     ]
    }
   ],
   "source": [
    "print(max([len(s) for s in card_strings]))\n",
    "\n",
    "inp = [encode_card_string(tokens, s) for s in card_strings]\n",
    "outp = [encode_card_string_nested(tokens, s) for s in card_strings]\n",
    "\n",
    "inp2 = np.squeeze(np.asarray(inp))\n",
    "outp2 = np.asarray(outp)\n",
    "print(.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "16209/16209 [==============================] - 624s 38ms/step - loss: 1.1804\n"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=[inp2, inp2],\n",
    "    y=outp2,\n",
    "    epochs=1,\n",
    ")\n",
    "\n",
    "r = model.predict([inp2[0:1], inp2[0:1]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 200, 57)\n",
      "(1, 200)\n",
      "[24 41 43 32 29 24 26 43 51 24 42  2 35 38 37 30  2 24 42  2 41  2 32 42\n",
      "  2 44 37 43 24 39 39 28 27  8  2 39 35 24 48 28 41 42  2 26 24 37 43 43\n",
      "  2 44 37 43 24 39  2 36 38 41 28  2 43 31 24 37  2 43 46 38  2 39 28 41\n",
      " 36 24 37 28 37 43 42  2 27 44 41 32 37 30  2 43 31 28 32 41  2 44 37 43\n",
      " 24 39  2 42 43 28 39 42 10 51 50 15 52  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifact|as long as r is untapped, players cantt untap more than two permanents during their untap steps.|{3}<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(r.shape)\n",
    "r2 = np.argmax(r, axis=2)\n",
    "print(r2.shape)\n",
    "(print(r2[0]))\n",
    "\"\".join([tokens[i] for i in r2[0] if i < len(tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Decoder-Input', 'Encoder-Input', 'Token-Embedding', 'Encoder-Embedding', 'Encoder-1-MultiHeadSelfAttention', 'Encoder-1-MultiHeadSelfAttention-Dropout', 'Encoder-1-MultiHeadSelfAttention-Add', 'Encoder-1-MultiHeadSelfAttention-Norm', 'Encoder-1-FeedForward', 'Encoder-1-FeedForward-Dropout', 'Encoder-1-FeedForward-Add', 'Encoder-1-FeedForward-Norm', 'Encoder-2-MultiHeadSelfAttention', 'Encoder-2-MultiHeadSelfAttention-Dropout', 'Encoder-2-MultiHeadSelfAttention-Add', 'Encoder-2-MultiHeadSelfAttention-Norm', 'Encoder-2-FeedForward', 'Encoder-2-FeedForward-Dropout', 'Encoder-2-FeedForward-Add', 'Encoder-2-FeedForward-Norm', 'Encoder-3-MultiHeadSelfAttention', 'Encoder-3-MultiHeadSelfAttention-Dropout', 'Encoder-3-MultiHeadSelfAttention-Add', 'Encoder-3-MultiHeadSelfAttention-Norm', 'Encoder-3-FeedForward', 'Encoder-3-FeedForward-Dropout', 'Encoder-3-FeedForward-Add', 'Decoder-Embedding', 'Encoder-3-FeedForward-Norm', 'Decoder-1-MultiHeadSelfAttention', 'reshape_3', 'Decoder-1-MultiHeadSelfAttention-Dropout', 'compr', 'Decoder-1-MultiHeadSelfAttention-Add', 'dld', 'Decoder-1-MultiHeadSelfAttention-Norm', 'reshape_4', 'Decoder-1-MultiHeadQueryAttention', 'Decoder-1-MultiHeadQueryAttention-Dropout', 'Decoder-1-MultiHeadQueryAttention-Add', 'Decoder-1-MultiHeadQueryAttention-Norm', 'Decoder-1-FeedForward', 'Decoder-1-FeedForward-Dropout', 'Decoder-1-FeedForward-Add', 'Decoder-1-FeedForward-Norm', 'Decoder-2-MultiHeadSelfAttention', 'Decoder-2-MultiHeadSelfAttention-Dropout', 'Decoder-2-MultiHeadSelfAttention-Add', 'Decoder-2-MultiHeadSelfAttention-Norm', 'Decoder-2-MultiHeadQueryAttention', 'Decoder-2-MultiHeadQueryAttention-Dropout', 'Decoder-2-MultiHeadQueryAttention-Add', 'Decoder-2-MultiHeadQueryAttention-Norm', 'Decoder-2-FeedForward', 'Decoder-2-FeedForward-Dropout', 'Decoder-2-FeedForward-Add', 'Decoder-2-FeedForward-Norm', 'Decoder-Output']\n"
     ]
    }
   ],
   "source": [
    "print([layer.name for layer in model.layers])\n",
    "extractors = keras.Model(inputs=model.inputs,\n",
    "                         outputs=model.get_layer(\"compr\").output)\n",
    "\n",
    "ex2 = keras.Model(inputs=model.inputs,\n",
    "                        outputs=model.get_layer(\"Encoder-3-FeedForward-Norm\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = extractors.predict([inp2[0:1], inp2[0:1]])\n",
    "foo2 = ex2.predict([inp2[0:1], inp2[0:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.370827  18.738321  -0.        25.030046  27.198847  -0.\n",
      "  15.898533   7.2908792 16.212776  22.5635    10.603739  28.726414 ]]\n"
     ]
    }
   ],
   "source": [
    "print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
